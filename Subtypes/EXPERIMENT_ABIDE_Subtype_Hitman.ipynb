{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Hits\n",
    "and write them out to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,3,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "mtp_list = ['rmap_part', 'dual_regression', 'stability_maps']\n",
    "pheno_list = [\n",
    "              '/data1/abide/Pheno/combined_406.csv',\n",
    "              '/data1/abide/Pheno/site_balanced_279.csv',\n",
    "              '/data1/abide/Pheno/site_and_age_balanced_194.csv'\n",
    "              ]\n",
    "sample_names = [\n",
    "                'combined_406_sample',\n",
    "                'site_279_sample',\n",
    "                'site_age_194_sample'\n",
    "                ]\n",
    "scale_list = np.array([7, 12, 20, 36, 64])\n",
    "#cov_list = ['VIQ', 'DX_GROUP', 'ADOS_SOCOM_SEV', 'VPR', 'EYE_STATUS_AT_SCAN']\n",
    "cov_list = ['EYE_STATUS_AT_SCAN', 'ADOS_SOCOM_SEV']\n",
    "out_str = 'Network,VIQ,DX_GROUP,ADOS_SOCOM_SEV,VPR,EYE_STATUS_AT_SCAN'\n",
    "# Select\n",
    "mtp_id = 2\n",
    "sc_id = [0,1,2]\n",
    "phen_id = 1\n",
    "# Make variables\n",
    "scales = scale_list[sc_id]\n",
    "mtp = mtp_list[mtp_id]\n",
    "name = sample_names[phen_id]\n",
    "pheno_path = pheno_list[phen_id]\n",
    "# Fixed values\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "in_path = '/data1/subtypes/serial_preps/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cases = len(pheno)\n",
    "pheno['rand'] = np.arange(n_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomize the covariate\n",
    "tmp = pheno[cov_list].values\n",
    "np.random.shuffle(tmp)\n",
    "pheno[cov_list] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SCALE 7 for stability_maps with site_279_sample\n",
      "\n",
      "    0 findings for EYE_STATUS_AT_SCAN\n",
      "\n",
      "    0 findings for ADOS_SOCOM_SEV\n",
      "\n",
      "\n",
      "SCALE 12 for stability_maps with site_279_sample\n",
      "\n",
      "    0 findings for EYE_STATUS_AT_SCAN\n",
      "\n",
      "    0 findings for ADOS_SOCOM_SEV\n",
      "\n",
      "\n",
      "SCALE 20 for stability_maps with site_279_sample\n",
      "\n",
      "    0 findings for EYE_STATUS_AT_SCAN\n",
      "\n",
      "    0 findings for ADOS_SOCOM_SEV\n"
     ]
    }
   ],
   "source": [
    "for scale in scales:\n",
    "    print('\\n\\nSCALE {} for {} with {}'.format(scale, mtp, name))\n",
    "    # Scale stuff\n",
    "    netstack_path = os.path.join(in_path, 'netstack_dmn_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    netraw_path = os.path.join(in_path, 'netstack_raw_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    corrmat_path = os.path.join(in_path, 'correlation_matrix_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "    \n",
    "    # Get the prior\n",
    "    p_img = nib.load(prior_path)\n",
    "    prior = p_img.get_data()\n",
    "    \n",
    "    # Turn the priors into an image\n",
    "    prior = nib.load(prior_path)\n",
    "    prior_data = prior.get_data()\n",
    "    prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "    for sc_id in range(scale):\n",
    "        tmp = np.zeros_like(prior_data)\n",
    "        tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "        prior_temp[..., sc_id] = tmp\n",
    "    prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    \n",
    "    # Load the serialized netstack\n",
    "    netstack = np.load(netstack_path)\n",
    "    corr_mat = np.load(corrmat_path)\n",
    "    \n",
    "    subtypes = 5\n",
    "\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    # Make the grand average\n",
    "    gdavg = np.zeros(mask.shape + (scale,))\n",
    "\n",
    "    scale = netstack.shape[0]\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    link_store = np.zeros((n_sub-1,4,scale))\n",
    "    part_store = np.zeros((scale, n_sub))\n",
    "    sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "    weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "    # Iterate through the networks\n",
    "    for net_id in range(scale):\n",
    "        # Compute linkage with Ward's criterion\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "        link_store[..., net_id] = link_mat\n",
    "        # Partition the linkage to get a given number of subtypes\n",
    "        part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "        part_store[net_id, :] = part_sub\n",
    "\n",
    "        sub_stack = np.zeros((n_vox, subtypes))\n",
    "        for s_id in range(subtypes):\n",
    "            sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "            sub_stack[:,s_id] = sbt\n",
    "            sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "    for cov in cov_list:\n",
    "        cov_index = pd.notnull(pheno.replace(-9999, np.nan)[cov])\n",
    "        cov_pheno = pheno[cov_index]\n",
    "        # Generate the model matrix\n",
    "        factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "        # Make dummy variables for the site factor\n",
    "        site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "        # Turn the first site into the intercept\n",
    "        site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "        site_factor['INTERCEPT'] = 1\n",
    "        # Get the other variables\n",
    "        other_factors = cov_pheno.ix[:,factors]\n",
    "        # Turn diagnosis into [0,1] vector\n",
    "        #other_factors['DX_GROUP'] = other_factors['DX_GROUP'].values - 1\n",
    "        # Demean age\n",
    "        other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "        # Demean the covariate\n",
    "        other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "        # Put them back together\n",
    "        glm_pheno = pd.concat([site_factor, other_factors], axis=1)\n",
    "        cov_weight = weight_store[..., cov_index.values]\n",
    "        res_store = list()\n",
    "        pval_store = np.zeros((scale, subtypes))\n",
    "        for net_id in range(scale):\n",
    "            res_list = list()\n",
    "            # Loop through the subtypes\n",
    "            for s_id in range(subtypes):\n",
    "                model = sm.OLS(cov_weight[net_id, s_id, :], glm_pheno)\n",
    "                results = model.fit()\n",
    "                # Save the p-values\n",
    "                pval_store[net_id, s_id] = results.pvalues[cov]\n",
    "                res_list.append(results)\n",
    "            res_store.append(res_list)\n",
    "        # Now look at the mask of p-values passing FDR Correction\n",
    "        pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "        pcorr_vec = smi.multipletests(pval_vec.flatten(), alpha=0.05, method='fdr_bh')\n",
    "        # Do a bonferroni correction too\n",
    "        bonfcrit = 0.05 / np.prod(pval_store.shape)\n",
    "        bonfvec = pval_vec < bonfcrit\n",
    "        bonfthresh = np.reshape(bonfvec, pval_store.shape)\n",
    "\n",
    "        # pcorr_vec = sts.multicomp.fdrcorrection0(pval_vec, 0.05)\n",
    "        # Find the hits\n",
    "        if np.sum(pcorr_vec[0]) > 0:\n",
    "        #if bonfthresh.any():\n",
    "            pcorr_store = np.reshape(pcorr_vec[0], pval_store.shape)\n",
    "            hits = np.argwhere(pcorr_store!=0)\n",
    "            print('\\n    {} findings for {} ({})'.format(np.sum(pcorr_vec[0]), cov, list(hits)))\n",
    "        else:\n",
    "            print('\\n    {} findings for {}'.format(np.sum(pcorr_vec[0]), cov))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
